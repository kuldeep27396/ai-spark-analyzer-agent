"""
Setup script for AI Spark Analyzer Airflow deployment
Configures BigQuery, GCS, and Airflow variables
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List
import argparse
import sys
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

try:
    from airflow.models import Variable
    from airflow.models.dagbag import DagBag
    from airflow.utils.db import create_session
except ImportError:
    print("‚ùå Airflow not installed. Please install Airflow first.")
    print("   pip install apache-airflow")
    print("   pip install apache-airflow-providers-google")
    sys.exit(1)

from core.config import Config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AirflowSetup:
    """Setup class for Airflow deployment"""

    def __init__(self, config_path: str = None):
        self.config = Config()
        self.setup_config = self._load_setup_config(config_path)

    def _load_setup_config(self, config_path: str) -> Dict[str, Any]:
        """Load setup configuration from file"""
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                return json.load(f)
        return {}

    def setup_all(self):
        """Setup all components"""
        logger.info("üöÄ Starting AI Spark Analyzer Airflow setup")

        # 1. Set Airflow variables
        self.setup_airflow_variables()

        # 2. Validate configuration
        self.validate_configuration()

        # 3. Create required directories
        self.create_directories()

        # 4. Generate example environment file
        self.generate_env_file()

        logger.info("‚úÖ Airflow setup completed successfully")

    def setup_airflow_variables(self):
        """Set required Airflow variables"""
        logger.info("‚öôÔ∏è Setting up Airflow variables")

        variables = {
            # BigQuery settings
            "spark_analyzer_bigquery_initialized": False,
            "spark_analyzer_retention_days": 365,

            # Email settings
            "spark_analyzer_email_enabled": True,
            "spark_analyzer_monthly_summary_day": 1,

            # Monitoring settings
            "spark_analyzer_monitoring_enabled": True,
            "spark_analyzer_alert_enabled": True,

            # Performance settings
            "spark_analyzer_daily_analysis_enabled": True,
            "spark_analyzer_continuous_mode": False,

            # Data retention
            "spark_analyzer_data_retention_days": 365,
            "spark_analyzer_log_retention_days": 30,

            # Notification settings
            "spark_analyzer_admin_emails": ",".join(self.setup_config.get("admin_emails", [])),
            "spark_analyzer_notification_emails": ",".join(self.setup_config.get("notification_emails", [])),
        }

        # Set variables in Airflow
        with create_session() as session:
            for key, value in variables.items():
                existing_var = session.query(Variable).filter(Variable.key == key).first()
                if existing_var:
                    existing_var.val = str(value)
                    logger.info(f"‚úÖ Updated variable: {key}")
                else:
                    new_var = Variable(key=key, val=str(value))
                    session.add(new_var)
                    logger.info(f"‚úÖ Created variable: {key}")

            session.commit()

        logger.info(f"‚úÖ Set {len(variables)} Airflow variables")

    def validate_configuration(self):
        """Validate configuration"""
        logger.info("üîç Validating configuration")

        required_configs = [
            ("GOOGLE_CLOUD_PROJECT", "Google Cloud Project ID"),
            ("OPENAI_API_KEY", "OpenAI API Key"),
            ("EMAIL_USERNAME", "Email Username"),
            ("EMAIL_PASSWORD", "Email Password"),
            ("EMAIL_FROM", "From Email Address"),
        ]

        missing_configs = []
        for env_var, description in required_configs:
            if not getattr(self.config, env_var.lower(), None):
                missing_configs.append(f"{env_var} ({description})")

        if missing_configs:
            logger.error("‚ùå Missing required configurations:")
            for config in missing_configs:
                logger.error(f"   - {config}")
            raise ValueError("Missing required configuration")

        logger.info("‚úÖ Configuration validation passed")

    def create_directories(self):
        """Create required directories"""
        logger.info("üìÅ Creating required directories")

        directories = [
            self.config.data_dir,
            self.config.log_dir,
            self.config.output_dir,
            self.config.database.vector_db_path,
            self.config.email.email_template_dir,
        ]

        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"‚úÖ Created directory: {directory}")

    def generate_env_file(self):
        """Generate example environment file"""
        env_file_path = Path(".env.example")

        env_content = f"""# AI Spark Analyzer Environment Configuration
# Generated on: {datetime.utcnow().isoformat()}

# Environment
ENVIRONMENT=production
LOG_LEVEL=INFO

# Google Cloud Configuration
GOOGLE_CLOUD_PROJECT={self.config.gcp.project_id}
DATAPROC_REGION={self.config.gcp.region}
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account.json

# BigQuery Configuration
BIGQUERY_DATASET={self.config.gcp.bigquery_dataset}
BIGQUERY_LOCATION={self.config.gcp.bigquery_location}

# GCS Configuration
GCS_BUCKET={self.config.gcp.gcs_bucket}
GCS_REPORTS_PREFIX={self.config.gcp.gcs_reports_prefix}

# AI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4-turbo-preview
AI_TEMPERATURE=0.1
AI_MAX_TOKENS=4000

# Email Configuration
EMAIL_USERNAME=your-email@company.com
EMAIL_PASSWORD=your-app-password
EMAIL_FROM=ai-spark-analyzer@company.com
EMAIL_ADMINS=admin@company.com,team-leads@company.com
EMAIL_NOTIFICATIONS=devops@company.com
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USE_TLS=true

# Monitoring Configuration
MONITORING_SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
MONITORING_SLACK_CHANNEL=#spark-alerts

# Analysis Configuration
ANALYSIS_DAILY_ANALYSIS_HOUR=9
ANALYSIS_RECOMMENDATION_FREQUENCY_DAYS=30
ANALYSIS_MIN_JOB_DURATION_SECONDS=60
ANALYSIS_COST_ANALYSIS_ENABLED=true
ANALYSIS_CPU_USAGE_THRESHOLD=80.0
ANALYSIS_MEMORY_USAGE_THRESHOLD=85.0

# Dashboard Configuration
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8080
DASHBOARD_DEBUG=false
DASHBOARD_SECRET_KEY=your-secret-key-here

# Data Paths
DATA_DIR=./data
LOG_DIR=./logs
OUTPUT_DIR=./output
"""

        with open(env_file_path, 'w') as f:
            f.write(env_content)

        logger.info(f"‚úÖ Generated environment file: {env_file_path}")
        logger.info("   Copy this to .env and update with your values")

    def create_airflow_connections(self):
        """Create required Airflow connections"""
        logger.info("üîå Setting up Airflow connections")

        connections = [
            {
                "conn_id": "google_cloud_default",
                "conn_type": "google_cloud_platform",
                "description": "Google Cloud Platform connection for BigQuery and GCS",
                "extra": json.dumps({
                    "key_path": self.config.gcp.credentials_path,
                    "project_id": self.config.gcp.project_id,
                    "location": self.config.gcp.region
                })
            },
            {
                "conn_id": "spark_analyzer_email",
                "conn_type": "smtp",
                "description": "Email connection for AI Spark Analyzer notifications",
                "host": self.config.email.smtp_server,
                "port": self.config.email.smtp_port,
                "login": self.config.email.username,
                "password": self.config.email.password,
                "extra": json.dumps({
                    "use_tls": self.config.email.use_tls,
                    "from_email": self.config.email.from_email
                })
            }
        ]

        # Import here to avoid circular imports
        try:
            from airflow.providers.google.common.operators.base_google import GoogleBaseOperator
            from airflow.providers.smtp.operators.smtp import SMTPOperator
        except ImportError:
            logger.warning("‚ö†Ô∏è Cannot create connections - missing provider packages")
            return

        with create_session() as session:
            # This would need actual connection management code
            # For now, just log the connection requirements
            for conn in connections:
                logger.info(f"üìã Connection required: {conn['conn_id']} ({conn['conn_type']})")
                logger.info(f"   Description: {conn['description']}")

        logger.info("‚úÖ Connection requirements documented")

    def test_configuration(self):
        """Test the configuration"""
        logger.info("üß™ Testing configuration")

        try:
            # Test BigQuery connection
            from google.cloud import bigquery
            client = bigquery.Client(project=self.config.gcp.project_id)
            dataset_ref = client.dataset(self.config.gcp.bigquery_dataset)
            logger.info("‚úÖ BigQuery connection successful")

            # Test email configuration (optional)
            # This would require actual email testing code
            logger.info("‚úÖ Email configuration loaded")

        except Exception as e:
            logger.error(f"‚ùå Configuration test failed: {e}")
            raise

        logger.info("‚úÖ Configuration test passed")

    def deploy_dag(self):
        """Deploy the DAG to Airflow"""
        logger.info("üöÄ Deploying DAG to Airflow")

        dag_file = Path(__file__).parent.parent / "dags" / "spark_analyzer_dag.py"

        if not dag_file.exists():
            raise FileNotFoundError(f"DAG file not found: {dag_file}")

        # Validate DAG
        try:
            dag_bag = DagBag(dag_folder=str(dag_file.parent))
            dag_id = "ai_spark_analyzer_daily"

            if dag_id not in dag_bag.dags:
                raise ValueError(f"DAG {dag_id} not found in DAG bag")

            dag = dag_bag.dags[dag_id]
            logger.info(f"‚úÖ DAG {dag_id} validated successfully")
            logger.info(f"   Schedule: {dag.schedule_interval}")
            logger.info(f"   Tasks: {len(dag.tasks)}")
            logger.info(f"   Owner: {dag.owner}")

        except Exception as e:
            logger.error(f"‚ùå DAG validation failed: {e}")
            raise

        logger.info("‚úÖ DAG deployment ready")
        logger.info("   Copy the DAG file to your Airflow dags directory")

    def generate_deployment_docs(self):
        """Generate deployment documentation"""
        logger.info("üìö Generating deployment documentation")

        docs = f"""# AI Spark Analyzer Airflow Deployment Guide

## Overview
This guide covers deploying the AI Spark Analyzer with Airflow for daily execution, BigQuery storage, and 30-day email summaries.

## Prerequisites

### 1. Google Cloud Setup
- Google Cloud Project with billing enabled
- BigQuery API enabled
- Cloud Storage API enabled
- Dataproc API enabled

### 2. Service Account
Create a service account with the following roles:
- BigQuery Data Editor
- BigQuery Job User
- Storage Object Admin
- Dataproc Viewer

### 3. Airflow Setup
- Airflow {self.setup_config.get('airflow_version', '2.5+')} installed
- Google Cloud Provider: `pip install apache-airflow-providers-google`
- SMTP Provider: `pip install apache-airflow-providers-smtp`

## Configuration

### 1. Environment Variables
Set the following environment variables:

```bash
# Google Cloud
export GOOGLE_CLOUD_PROJECT="{self.config.gcp.project_id}"
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"

# AI Configuration
export OPENAI_API_KEY="your-openai-api-key"

# Email Configuration
export EMAIL_USERNAME="your-email@company.com"
export EMAIL_PASSWORD="your-app-password"
export EMAIL_FROM="ai-spark-analyzer@company.com"
export EMAIL_ADMINS="admin@company.com,team@company.com"
```

### 2. Airflow Variables
The following Airflow variables will be automatically set:
- spark_analyzer_bigquery_initialized: false
- spark_analyzer_retention_days: 365
- spark_analyzer_email_enabled: true
- spark_analyzer_monthly_summary_day: 1

### 3. Airflow Connections
Create these Airflow connections:
- google_cloud_default: GCP connection for BigQuery/GCS
- spark_analyzer_email: SMTP connection for notifications

## BigQuery Schema

The system creates these BigQuery tables in `{self.config.gcp.project_id}.{self.config.gcp.bigquery_dataset}`:

1. **job_analysis** - Job execution data (partitioned by date)
2. **patterns** - Identified patterns and anomalies
3. **recommendations** - Optimization recommendations
4. **optimization_plans** - 30-day optimization plans
5. **daily_summaries** - Daily analysis summaries
6. **job_feedback** - User feedback on recommendations

## DAG Schedule

- **Frequency**: Daily at midnight UTC
- **Email Summaries**: Sent on the 1st of each month
- **Data Retention**: 365 days (configurable)

## Monitoring

### 1. Airflow Monitoring
- Monitor DAG runs in Airflow UI
- Set up alerts for DAG failures
- Track task execution times

### 2. BigQuery Monitoring
- Monitor storage costs
- Set up cost alerts
- Review query performance

### 3. Email Monitoring
- Track email delivery success
- Monitor bounce rates
- Set up alerts for email failures

## Troubleshooting

### Common Issues

1. **BigQuery Permissions**
   - Ensure service account has BigQuery Data Editor role
   - Check dataset exists and service account has access

2. **Email Delivery**
   - Verify SMTP credentials
   - Check firewall/SMTP relay settings
   - Review email quota limits

3. **DAG Failures**
   - Check Airflow logs for detailed error messages
   - Verify all environment variables are set
   - Ensure all required packages are installed

### Log Locations
- Airflow logs: `$AIRFLOW_HOME/logs`
- Application logs: `{self.config.log_dir}`
- BigQuery logs: Google Cloud Console

## Security Considerations

1. **Service Account Security**
   - Use principle of least privilege
   - Rotate service account keys regularly
   - Store credentials securely

2. **Email Security**
   - Use app-specific passwords for Gmail
   - Consider OAuth2 for production
   - Monitor email account security

3. **Data Security**
   - Enable BigQuery encryption
   - Use IAM controls for data access
   - Regularly review access logs

## Maintenance

### Monthly Tasks
- Review email summary delivery
- Check BigQuery storage costs
- Update retention policies as needed
- Review and rotate credentials

### Quarterly Tasks
- Optimize BigQuery partitions
- Review DAG performance
- Update packages and dependencies
- Review alert configurations

## Support

For issues and questions:
1. Check Airflow logs for detailed error messages
2. Review BigQuery job history
3. Monitor email delivery reports
4. Contact your CloudOps team for infrastructure issues

Generated on: {datetime.utcnow().isoformat()}
"""

        docs_path = Path("DEPLOYMENT.md")
        with open(docs_path, 'w') as f:
            f.write(docs)

        logger.info(f"‚úÖ Deployment documentation generated: {docs_path}")


def main():
    """Main setup function"""
    parser = argparse.ArgumentParser(
        description="Setup AI Spark Analyzer for Airflow deployment"
    )
    parser.add_argument(
        "--config",
        help="Path to setup configuration JSON file"
    )
    parser.add_argument(
        "--test-config",
        action="store_true",
        help="Test configuration after setup"
    )
    parser.add_argument(
        "--deploy-dag",
        action="store_true",
        help="Validate and prepare DAG for deployment"
    )
    parser.add_argument(
        "--create-connections",
        action="store_true",
        help="Create Airflow connections (documentation only)"
    )
    parser.add_argument(
        "--docs",
        action="store_true",
        help="Generate deployment documentation"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Run complete setup process"
    )

    args = parser.parse_args()

    try:
        setup = AirflowSetup(args.config)

        if args.all or len(sys.argv) == 1:
            # Run full setup
            setup.setup_all()
            setup.generate_deployment_docs()

            if args.test_config or args.all:
                setup.test_configuration()

            if args.deploy_dag or args.all:
                setup.deploy_dag()

        else:
            # Run specific components
            setup.setup_airflow_variables()
            setup.validate_configuration()
            setup.create_directories()
            setup.generate_env_file()

            if args.test_config:
                setup.test_configuration()

            if args.deploy_dag:
                setup.deploy_dag()

            if args.create_connections:
                setup.create_airflow_connections()

            if args.docs:
                setup.generate_deployment_docs()

        print("\nüéâ Setup completed successfully!")
        print("\nüìã Next steps:")
        print("1. Update the generated .env file with your values")
        print("2. Copy the DAG to your Airflow dags directory")
        print("3. Set up Airflow connections in the UI")
        print("4. Enable the DAG in Airflow")
        print("5. Monitor the first DAG run")

    except Exception as e:
        logger.error(f"‚ùå Setup failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()